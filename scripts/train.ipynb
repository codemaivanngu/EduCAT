{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tung\\.conda\\envs\\EduCAT\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import CAT\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setuplogger():\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"[%(levelname)s %(asctime)s] %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    root.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "setuplogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose dataset here\n",
    "dataset = 'assistment'\n",
    "# modify config here\n",
    "config = {\n",
    "    'learning_rate': 0.002,\n",
    "    'batch_size': 2048,\n",
    "    'num_epochs': 10,\n",
    "    'num_dim': 1, # for IRT or MIRT\n",
    "    'device': 'cpu',\n",
    "    # for NeuralCD\n",
    "    'prednet_len1': 128,\n",
    "    'prednet_len2': 64,\n",
    "    'betas': (0.9, 0.999),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets\n",
    "# train_triplets = pd.read_csv(f'../data/{dataset}/train_triples.csv', encoding='utf-8').to_records(index=False)\n",
    "train_triplets = pd.read_csv(f'dataset/train_triples.csv', encoding='utf-8').to_records(index=False)\n",
    "# concept_map = json.load(open(f'../data/{dataset}/concept_map.json', 'r'))\n",
    "concept_map = json.load(open(f'dataset/concept_map.json', 'r'))\n",
    "concept_map = {int(k):v for k,v in concept_map.items()}\n",
    "# metadata = json.load(open(f'../data/{dataset}/metadata.json', 'r'))\n",
    "metadata = json.load(open(f'dataset/metadata.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CAT.dataset.TrainDataset(train_triplets, concept_map,\n",
    "                                      metadata['num_train_students'], \n",
    "                                      metadata['num_questions'], \n",
    "                                      metadata['num_concepts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CAT.dataset.TrainDataset(train_triplets, concept_map,\n",
    "                                      metadata['num_train_students'], \n",
    "                                      metadata['num_questions'], \n",
    "                                      metadata['num_concepts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO 2025-02-24 17:10:11,012] train on cpu\n",
      "[INFO 2025-02-24 17:10:11,129] Epoch [1] Batch [0]: loss=inf\n",
      "[INFO 2025-02-24 17:10:11,797] Epoch [1] Batch [10]: loss=0.76152\n",
      "[INFO 2025-02-24 17:10:12,479] Epoch [1] Batch [20]: loss=0.72584\n",
      "[INFO 2025-02-24 17:10:13,313] Epoch [1] Batch [30]: loss=0.71351\n",
      "[INFO 2025-02-24 17:10:13,963] Epoch [1] Batch [40]: loss=0.70692\n",
      "[INFO 2025-02-24 17:10:14,664] Epoch [1] Batch [50]: loss=0.70266\n",
      "[INFO 2025-02-24 17:10:15,443] Epoch [1] Batch [60]: loss=0.69959\n",
      "[INFO 2025-02-24 17:10:16,148] Epoch [1] Batch [70]: loss=0.69716\n",
      "[INFO 2025-02-24 17:10:16,905] Epoch [1] Batch [80]: loss=0.69515\n",
      "[INFO 2025-02-24 17:10:17,622] Epoch [1] Batch [90]: loss=0.69344\n",
      "[INFO 2025-02-24 17:10:18,299] Epoch [2] Batch [0]: loss=inf\n",
      "[INFO 2025-02-24 17:10:19,101] Epoch [2] Batch [10]: loss=0.74401\n",
      "[INFO 2025-02-24 17:10:19,774] Epoch [2] Batch [20]: loss=0.70949\n",
      "[INFO 2025-02-24 17:10:20,483] Epoch [2] Batch [30]: loss=0.69732\n",
      "[INFO 2025-02-24 17:10:21,384] Epoch [2] Batch [40]: loss=0.69097\n",
      "[INFO 2025-02-24 17:10:22,138] Epoch [2] Batch [50]: loss=0.68683\n",
      "[INFO 2025-02-24 17:10:22,858] Epoch [2] Batch [60]: loss=0.68391\n",
      "[INFO 2025-02-24 17:10:23,569] Epoch [2] Batch [70]: loss=0.68144\n",
      "[INFO 2025-02-24 17:10:24,239] Epoch [2] Batch [80]: loss=0.67945\n",
      "[INFO 2025-02-24 17:10:24,891] Epoch [2] Batch [90]: loss=0.67777\n",
      "[INFO 2025-02-24 17:10:25,505] Epoch [3] Batch [0]: loss=inf\n",
      "[INFO 2025-02-24 17:10:26,252] Epoch [3] Batch [10]: loss=0.72598\n",
      "[INFO 2025-02-24 17:10:26,937] Epoch [3] Batch [20]: loss=0.69257\n",
      "[INFO 2025-02-24 17:10:27,738] Epoch [3] Batch [30]: loss=0.68102\n",
      "[INFO 2025-02-24 17:10:28,505] Epoch [3] Batch [40]: loss=0.67464\n",
      "[INFO 2025-02-24 17:10:29,237] Epoch [3] Batch [50]: loss=0.67051\n",
      "[INFO 2025-02-24 17:10:30,029] Epoch [3] Batch [60]: loss=0.66755\n",
      "[INFO 2025-02-24 17:10:30,832] Epoch [3] Batch [70]: loss=0.66495\n",
      "[INFO 2025-02-24 17:10:31,838] Epoch [3] Batch [80]: loss=0.66279\n",
      "[INFO 2025-02-24 17:10:32,639] Epoch [3] Batch [90]: loss=0.66100\n",
      "[INFO 2025-02-24 17:10:33,375] Epoch [4] Batch [0]: loss=inf\n",
      "[INFO 2025-02-24 17:10:34,065] Epoch [4] Batch [10]: loss=0.70738\n",
      "[INFO 2025-02-24 17:10:34,872] Epoch [4] Batch [20]: loss=0.67400\n",
      "[INFO 2025-02-24 17:10:35,623] Epoch [4] Batch [30]: loss=0.66228\n",
      "[INFO 2025-02-24 17:10:36,306] Epoch [4] Batch [40]: loss=0.65612\n",
      "[INFO 2025-02-24 17:10:37,043] Epoch [4] Batch [50]: loss=0.65180\n",
      "[INFO 2025-02-24 17:10:37,691] Epoch [4] Batch [60]: loss=0.64880\n",
      "[INFO 2025-02-24 17:10:38,360] Epoch [4] Batch [70]: loss=0.64633\n",
      "[INFO 2025-02-24 17:10:39,234] Epoch [4] Batch [80]: loss=0.64443\n",
      "[INFO 2025-02-24 17:10:40,069] Epoch [4] Batch [90]: loss=0.64265\n",
      "[INFO 2025-02-24 17:10:40,725] Epoch [5] Batch [0]: loss=inf\n",
      "[INFO 2025-02-24 17:10:41,624] Epoch [5] Batch [10]: loss=0.68629\n",
      "[INFO 2025-02-24 17:10:42,342] Epoch [5] Batch [20]: loss=0.65407\n",
      "[INFO 2025-02-24 17:10:43,027] Epoch [5] Batch [30]: loss=0.64302\n",
      "[INFO 2025-02-24 17:10:43,693] Epoch [5] Batch [40]: loss=0.63671\n",
      "[INFO 2025-02-24 17:10:44,459] Epoch [5] Batch [50]: loss=0.63281\n",
      "[INFO 2025-02-24 17:10:45,058] Epoch [5] Batch [60]: loss=0.63047\n",
      "[INFO 2025-02-24 17:10:45,805] Epoch [5] Batch [70]: loss=0.62820\n",
      "[INFO 2025-02-24 17:10:46,647] Epoch [5] Batch [80]: loss=0.62626\n",
      "[INFO 2025-02-24 17:10:47,296] Epoch [5] Batch [90]: loss=0.62444\n",
      "[INFO 2025-02-24 17:10:47,965] Epoch [6] Batch [0]: loss=inf\n",
      "[INFO 2025-02-24 17:10:48,734] Epoch [6] Batch [10]: loss=0.66692\n",
      "[INFO 2025-02-24 17:10:49,528] Epoch [6] Batch [20]: loss=0.63581\n",
      "[INFO 2025-02-24 17:10:50,498] Epoch [6] Batch [30]: loss=0.62494\n",
      "[INFO 2025-02-24 17:10:51,596] Epoch [6] Batch [40]: loss=0.61955\n",
      "[INFO 2025-02-24 17:10:52,417] Epoch [6] Batch [50]: loss=0.61643\n",
      "[INFO 2025-02-24 17:10:53,134] Epoch [6] Batch [60]: loss=0.61356\n",
      "[INFO 2025-02-24 17:10:53,902] Epoch [6] Batch [70]: loss=0.61104\n",
      "[INFO 2025-02-24 17:10:54,983] Epoch [6] Batch [80]: loss=0.60928\n",
      "[INFO 2025-02-24 17:10:55,704] Epoch [6] Batch [90]: loss=0.60776\n",
      "[INFO 2025-02-24 17:10:56,513] Epoch [7] Batch [0]: loss=inf\n",
      "[INFO 2025-02-24 17:10:57,473] Epoch [7] Batch [10]: loss=0.65034\n",
      "[INFO 2025-02-24 17:10:58,415] Epoch [7] Batch [20]: loss=0.61980\n",
      "[INFO 2025-02-24 17:10:59,151] Epoch [7] Batch [30]: loss=0.60927\n",
      "[INFO 2025-02-24 17:11:00,079] Epoch [7] Batch [40]: loss=0.60420\n",
      "[INFO 2025-02-24 17:11:00,772] Epoch [7] Batch [50]: loss=0.60077\n",
      "[INFO 2025-02-24 17:11:02,048] Epoch [7] Batch [60]: loss=0.59869\n",
      "[INFO 2025-02-24 17:11:03,213] Epoch [7] Batch [70]: loss=0.59672\n",
      "[INFO 2025-02-24 17:11:04,140] Epoch [7] Batch [80]: loss=0.59522\n",
      "[INFO 2025-02-24 17:11:04,874] Epoch [7] Batch [90]: loss=0.59403\n",
      "[INFO 2025-02-24 17:11:05,542] Epoch [8] Batch [0]: loss=inf\n",
      "[INFO 2025-02-24 17:11:06,318] Epoch [8] Batch [10]: loss=0.63840\n",
      "[INFO 2025-02-24 17:11:07,040] Epoch [8] Batch [20]: loss=0.60839\n",
      "[INFO 2025-02-24 17:11:07,669] Epoch [8] Batch [30]: loss=0.59789\n",
      "[INFO 2025-02-24 17:11:08,388] Epoch [8] Batch [40]: loss=0.59275\n",
      "[INFO 2025-02-24 17:11:09,090] Epoch [8] Batch [50]: loss=0.58994\n",
      "[INFO 2025-02-24 17:11:09,749] Epoch [8] Batch [60]: loss=0.58744\n",
      "[INFO 2025-02-24 17:11:10,440] Epoch [8] Batch [70]: loss=0.58610\n",
      "[INFO 2025-02-24 17:11:11,271] Epoch [8] Batch [80]: loss=0.58416\n",
      "[INFO 2025-02-24 17:11:12,033] Epoch [8] Batch [90]: loss=0.58276\n",
      "[INFO 2025-02-24 17:11:12,652] Epoch [9] Batch [0]: loss=inf\n",
      "[INFO 2025-02-24 17:11:13,414] Epoch [9] Batch [10]: loss=0.62797\n",
      "[INFO 2025-02-24 17:11:14,472] Epoch [9] Batch [20]: loss=0.59830\n",
      "[INFO 2025-02-24 17:11:15,415] Epoch [9] Batch [30]: loss=0.58821\n",
      "[INFO 2025-02-24 17:11:16,171] Epoch [9] Batch [40]: loss=0.58294\n",
      "[INFO 2025-02-24 17:11:16,956] Epoch [9] Batch [50]: loss=0.57983\n",
      "[INFO 2025-02-24 17:11:17,787] Epoch [9] Batch [60]: loss=0.57750\n",
      "[INFO 2025-02-24 17:11:18,678] Epoch [9] Batch [70]: loss=0.57574\n",
      "[INFO 2025-02-24 17:11:19,962] Epoch [9] Batch [80]: loss=0.57472\n",
      "[INFO 2025-02-24 17:11:20,859] Epoch [9] Batch [90]: loss=0.57357\n",
      "[INFO 2025-02-24 17:11:21,460] Epoch [10] Batch [0]: loss=inf\n",
      "[INFO 2025-02-24 17:11:22,226] Epoch [10] Batch [10]: loss=0.61879\n",
      "[INFO 2025-02-24 17:11:22,891] Epoch [10] Batch [20]: loss=0.59049\n",
      "[INFO 2025-02-24 17:11:23,608] Epoch [10] Batch [30]: loss=0.57969\n",
      "[INFO 2025-02-24 17:11:24,375] Epoch [10] Batch [40]: loss=0.57348\n",
      "[INFO 2025-02-24 17:11:25,085] Epoch [10] Batch [50]: loss=0.57037\n",
      "[INFO 2025-02-24 17:11:25,691] Epoch [10] Batch [60]: loss=0.56811\n",
      "[INFO 2025-02-24 17:11:26,632] Epoch [10] Batch [70]: loss=0.56689\n",
      "[INFO 2025-02-24 17:11:27,397] Epoch [10] Batch [80]: loss=0.56617\n",
      "[INFO 2025-02-24 17:11:28,259] Epoch [10] Batch [90]: loss=0.56557\n"
     ]
    }
   ],
   "source": [
    "# define model here\n",
    "model = CAT.model.IRTModel(**config)\n",
    "# train model\n",
    "model.init_model(train_data)\n",
    "model.train(train_data, log_step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.adaptest_save('../ckpt/irt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EduCAT)",
   "language": "python",
   "name": "educat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
